{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuEaFICjER-b"
   },
   "source": [
    "## Практическое задание\n",
    "## Метод обратного распространения ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FY3CK9LwER-d"
   },
   "source": [
    "### О задании\n",
    "\n",
    "В этом задание вы:\n",
    "* познакомитесь с методом обратного распространения ошибки \n",
    "* реализуете прямой проход и обратный проход в нейросети\n",
    "* реальзуете стохастический градиентный спуск с моментов\n",
    "* обучите нейросеть для классификации рукописных цифр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "se5_JXSqER-e"
   },
   "source": [
    "# Часть 1. Прямой и обратный проход нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "8tkSIyD7ER-f"
   },
   "source": [
    "## Теоретическая часть\n",
    "\n",
    "<p>\n",
    "Метод обратного распространения ошибки — это метод обучения многослойной нейрононной сети, который впервые был открыт двумя независимыми группами исследователей в 1974 г. Этот метод определяет алгоритм эффективного вычисления градиентов параметров нейронной сети, что позволяется применить метод градиентного спуска в задачи минимизации функционала ошибки. \n",
    "</p>\n",
    "\n",
    "Давайте рассморим M-слойную полносвязную нейронную сеть.\n",
    "\n",
    "<img src=\"network.png\" width=\"800\">\n",
    "\n",
    "На рисунке верхний индекс всегда используется для обозначения номера слоя нейросети. Рассмотрим некоторый n-ый слой, который назовем текущим. Данный слой на вход принимает\n",
    "$N^{(n-1)}$, \n",
    "признаков \n",
    "$y^{(n - 1)}_i$, $i=\\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n-1)}}$, \n",
    "которые являются значения выходов нейронов предыдущего слоя, и \n",
    "$y^{(n-1)}_0=1$\n",
    "(смещение или bias нейрона — константа, которая рассматривается как вход нейрона для упрощения записи дальнейших вычислений). Вес нейрона, связывающий $i$'ый нейрон предыдущего слоя с $j$'ым нейроном текущего, обозначен $w^{(n)}_{ij}$. \n",
    "За \n",
    "$z_j^{(n)}=\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$, \n",
    "обозначена линейная комбинация входов и весов.\n",
    "$\\sigma^{(n)}_j$ \n",
    "— функция активации j'ого нейрона(так как функции активации нейронов в общем случае могут быть различны), а \n",
    "$y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)}) = \\sigma_j^{(n)}(\\sum_{i=0}^{N^{(n-1)}}{w_{ij}^{(n)}y_i^{(n-1)}}) $ \n",
    "— значение функции активации или выход j'ого нейрона. \n",
    "\n",
    "<img src=\"layer.png\" width=\"800\">\n",
    "\n",
    "Резюмируем обозначения:\n",
    "* $M$ - колличество слоев\n",
    "* $N^{(n)}$ - колличество нейронов в $n$-ом слое\n",
    "* $\\{ w_{ij}^{(n)}\\}$ - веса нейронов $n$-ого слоя\n",
    "* $z_j^{(n)} = \\sum_{i=0}^{N^{(n - 1)}}{w_{ij}^{(n)}y_i^{(n-1)}}$\n",
    "* $\\sigma_j^{(n)}$ - функция активации $j$-ого нейрона $n$-ого слоя\n",
    "* $y_j^{(n)} = \\sigma_j^{(n)}(z_j^{(n)})$ - выход $j$-ого нейрона $n$-ого слоя \n",
    "\n",
    "За $E(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})$  обозначим некоторую диффенцируюмую функцию ошибки. Здесь  <br/>\n",
    "$\\boldsymbol{\\widehat{y}} = (y_1, ..., y_{N^{(M)}})$ - значение целевой переменной,  \n",
    "$\\boldsymbol{y} = (y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}))$ - выход нейросети. \n",
    "\n",
    "Давайте попробуем оценить сложность вычисление частной производной функции ошибки $E$. Предположим, что сложность вычисления частной производной функции в точке приблизительно равна сложности вычисления самой функции в точке. Пусть нейросеть имеет M полносвязных слоем по N нейронов в каждом слое. Тогда:\n",
    "* $O(N)$ - cложность вычисления одного выхода одного слоя (перемножение N весов на N входов)\n",
    "* $O(N^2)$ - cложность вычисления всех выходов одного слоя\n",
    "* $O(M * N^2)$ - cложность вычисления функции ошибки (последовательно вычисляем выходы M слоев)\n",
    "* $O(M^2 * N^4)$ - cложность частных производных функции ошибки по всем весам (всего  $O(M * N^2)$ весов)\n",
    "\n",
    "Получается для нейросети, состоящей из одного слоя с 1000 нейронами, сложность вычисления градиента должна быть равна $O(10^{12})$. А это уже невероятно много.\n",
    "\n",
    "Идея метода эффективного расчета градиентов заключается в том, чтобы при прямом проходе нейросети сохранить некоторые вычисленные значения, которые потом позволят быстро находить градиент."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mn2v5CX9ER-i"
   },
   "source": [
    "### Вычисление градиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "mzj0cAnWER-k"
   },
   "source": [
    "Будем вычислять частные производные функции ошибки от последних слоев в первым.\n",
    "Помимо частных производных \n",
    "$$\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$$ \n",
    "будем вычислять значения производных \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial y_i^{(n)}} E \\quad \\mathrm{и} \\quad \\frac{\\mathrm{\\partial}}{\\partial z_i^{(n)}} E$$ вычисление которых является одним из ключевых моментов в алгоритме обратном распространения ошибки. \n",
    "\n",
    "Полезно посмотреть как аналитически выглядит вычисления выхода нейросети \n",
    "$\\boldsymbol{\\widehat{y}}=(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)})$.:\n",
    "\n",
    "$$ \\widehat{y_j}=y_j^{(M)}=\\sigma_j^{(M)}(\\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})})$$\n",
    "\n",
    "В случае однослойной нейросети, получим: \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(1)}(\\sum_{i=0}^{N^{(0)}}{w_{ij}^{(1)}x_j}),$$\n",
    "\n",
    "двухслойной - \n",
    "\n",
    "$$ \\widehat{y_j}=\\sigma_j^{(2)}(\\sum_{i=0}^{N^{(1)}}{w_{ij}^{(2)}\\sigma_i^{(1)}(\\sum_{k=0}^{N^{(0)}}{w_{ki}^{(1)}x_k})}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRLHJVucER-m"
   },
   "source": [
    "**Вычисление градиента начнем с выходного слоя.** \n",
    "\n",
    "Вспомним как вычисляется выход:\n",
    "\n",
    "$$ y_j^{(M)}=\\sigma_j^{(M)}(z_j^{(M)})$$\n",
    "$$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$$\n",
    "\n",
    "Будем последовательно вычислять:\n",
    "\n",
    "1. $\\boldsymbol{\\frac{\\partial E}{\\partial y_j^{(M)}}}.$\n",
    "Так как функция E нам известна, то мы можем вычислить частные производные этой функции по переменным \n",
    "$y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}. $ \n",
    "Например, если\n",
    "$$E(\\boldsymbol{\\widehat{y}},  \\boldsymbol{y}^{(M)}) = \\frac{1}{2} \\| \\boldsymbol{\\widehat{y}} - \\boldsymbol{y}^{(M)} \\|_2^2 = \\frac{1}{2} \\sum_{j=1}^{N^{(M)}}{( \\widehat{y_j} - y_j^{(M)}) ^ 2},$$ \n",
    "то\n",
    "$$\\frac{\\partial E}{\\partial y_j^{(M)}} = y_j^{(M)} - \\widehat{y_j}$$\n",
    "Заметим, что в этом случае частная производная $E$ по $y_j^{(M)}$ равна ошибки на объекте $x_i$.\n",
    "\n",
    "2. $\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(y_1^{(M)}, ..., y_{N^{(M)}}^{(M)}) = E(y_1^{(M)}(z_1^{(M)}), ..., y_{N^{(M)}}^{(M)}(z_{N^{(M)}}^{(M)}))$.\n",
    "Вычислим \n",
    "$\\frac{\\mathrm{\\partial E}}{\\partial z_j^{(M)}}$ \n",
    "применив правило вычисление производной сложной функции \n",
    "$$\\frac{\\mathrm{\\partial}}{\\partial z_j^{(M)}} E = \\frac{\\partial E}{\\partial y_j^{(M)}} \\frac{\\partial y_j^{(M)}}{\\partial z_j^{(M)}} =  \\frac{\\partial E}{\\partial y_j^{(M)}} (\\sigma^{(M)}_j)' $$\n",
    "Важно, что $(\\sigma^{(M)}_j)'$ берется в точке \n",
    "$$z_j^{(M)}= \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}\\sigma_i^{(M-1)}(\\sum_{k=0}^{N^{(M-2)}}{w_{ki}^{(M-1)}\\sigma_k^{(M-2)}(...)})}.$$ \n",
    "Заметим что при прямом проходе, мы  вычисляем значение $z_j^{(M)}$, теперь дополнительно при прямом проходе будет сохранять это значение. Тогда вычисление \n",
    "$\\frac{\\mathrm{\\partial}}{\\partial z_i^{(M)}} E$ \n",
    "представляет собой вычисление значение  $(\\sigma^{(M)}_j)'$ в уже известной точке и перемножение двух чисел.\n",
    "\n",
    "3. $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(M)}}}.$ \n",
    "Функция ошибки \n",
    "$E = E(z_1^{(M)}, z_2^{(M)}, ..., z_{N^{(M)}}^{(M)})$. \n",
    "Вспомним, что \n",
    "$z_j^{(M)} = \\sum_{i=0}^{N^{(M-1)}}{w_{ij}^{(M)}y_i^{(M-1)}}$. \n",
    "Вес $w_{ij}^{(M)}$ входит только в одну сумму $z_j^{(M)}$. Тогда:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{ij}^{(M)}}= \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial z_j^{(M)}}{\\partial w_{ij}^{(M)}} =   \\frac{\\partial E}{\\partial z_j^{(M)}} \\frac{\\partial(\\sum_{k=0}^{N^{(M - 1)}}{w_{kj}^{(M)}y_k^{(M - 1)}}) }{\\partial w_{ij}^{(M)}} = \\frac{\\partial E}{\\partial z_j^{(M)}} y_i^{(M - 1)} $$\n",
    "\n",
    "Таким образом, мы  за 3 шага вычислили $\\frac{\\partial}{\\partial w_{ij}^{(n)}} E$ для последнего выходного слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eFiKo9p9ER-o"
   },
   "source": [
    "**Вычисление градиента произвольного внутренного слоя**\n",
    "\n",
    "Рассмотрим произвольный внутренний слой n. Вспомним что выходы этого слоя $y_i^{(n)}$ связаны с $z_j^{(n + 1)}$ следующего слоя соотношением:\n",
    "$$z_j^{(n + 1)}=\\sum_{i=0}^{N_{n}}{w_{ij}^{(n + 1)}y_i^{(n)}}, \\quad  j= \\overline{1\\mathinner {\\ldotp \\ldotp}N^{(n+1)}}$$\n",
    "Можно сказать, что \n",
    "$E = E(z_1^{(n + 1)}, z_2^{(n + 1)},..., z_{N^{(n+1)}}^{(n + 1)})$. \n",
    "А производные \n",
    "$\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}$ \n",
    "были посчитаны на предыдущем шаге.\n",
    "\n",
    "Тогда для \n",
    "$\\boldsymbol{\\frac{\\partial E}{\\partial y_i^{(n)}}}$ \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial z_{j}^{(n + 1)}}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} \\frac{\\partial (\\sum_{k=0}^{N_{n}}{w_{kj}^{(n + 1)}y_k^{(n)}})}{\\partial y_{i}^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} $$\n",
    "\n",
    "Эту величину, по аналогии с последним слоем будем называть ошибкой сети на скрытом слое. Заметим, что \n",
    "$$\\frac{\\partial E}{\\partial y_i^{(n)}} = \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial z_{j}^{(n + 1)}}} w_{ij}^{(n + 1)} =  \\sum_{j=1}^{N^{(n+1)}}{\\frac{\\partial E}{\\partial y_{j}^{(n + 1)}}}(\\sigma^{(n + 1)}_j)'  w_{ij}^{(n + 1)}.$$\n",
    "Таким образом, мы вычисляем ошибку текущего слоя через ошибку предыдущего, распространяя ее \"задом наперед\". Отсюда и название алгоритма — обратное распространение ошибок.\n",
    "\n",
    "Вычисление\n",
    "$\\boldsymbol{\\frac{\\mathrm{\\partial E}}{\\partial z_i^{(n)}}}$ и $\\boldsymbol{\\frac{\\partial E}{\\partial w_{ij}^{(n)}}}$ выполняется абсолютно аналогично последнему слою.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvsMqisUER-q"
   },
   "source": [
    "Таким образом, мы умеем последовательно вычислять частные производные по всем весам от последнего слоя к нейросети к первому. Заметим что все произодные можно быстро вычислять матрично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zkfmNpkER-r"
   },
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbt-TzC-ER-s"
   },
   "source": [
    "В этой части вам предстоит научиться:\n",
    "* вычислять производные среднеквадратичной функции ошибки и категориальной кросс энтропии\n",
    "* выполнять прямой и обратный проход неросети, состоящей из полносвязных слоей и функции антивации ReLu и Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIf5kVqWER-u"
   },
   "source": [
    "**Задание 1.** Вычисление производных функций ошибок\n",
    "\n",
    "Вам дан интерфейс класса `Loss`, вам нужно реализовать вычисление значение функции и ее градиента для среднеквадратичной ошибки $MSE$, а также для категориальной кросс-энтропии $H$. \n",
    "\n",
    "Пусть $\\widehat{y_i}$ - истинное значение функции, $y_i$ - предсказанное, тогда:\n",
    "$$MSE(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=\\frac{1}{d}\\sum_{i=1}^{d}{( \\widehat{y_i} - y_i)^2}$$\n",
    "$$H(\\boldsymbol{\\widehat{y}}, \\boldsymbol{y})=-\\frac{1}{d}\\sum_{i=1}^{d}{\\widehat{y_i}\\log({y_i})}$$\n",
    "\n",
    "Для численной устойчивости вам предлагается также реализовать градиент связки `softax + crossentropy`(в функции `gradient_with_sofmax`). Преобразование $softmax$ над вектором $x=(x_1, ..., x_d)$ можно записать как \n",
    "$$y_i = \\frac{e^{x_i}}{\\sum_{j=0}^{d}{e^{x_j}}}$$\n",
    "Функция `gradient_with_sofmax` должна возвращать \n",
    "$(\\frac{\\partial E}{\\partial x_1}, ..., \\frac{\\partial E}{\\partial x_d})$\n",
    "(Вам необходимо выполнить аналитические преобразования над $\\frac{\\partial E}{\\partial x_i}$ так, чтобы аналичически он зависел только от $\\boldsymbol{\\widehat{y}}$ и $\\boldsymbol{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1gRcb4cER-w"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "import base64\n",
    "import copy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FyTsIlVEER-3"
   },
   "outputs": [],
   "source": [
    "class Loss(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: loss\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true: np.array(d), ground truth (correct) labels\n",
    "        y_pred: np.array(d), estimated target values\n",
    "        ---\n",
    "        output: np.array(d), gradient loss to y_pred\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWKKtEygER-8"
   },
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return ((y_true - y_pred)**2).sum()/y_true.shape\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return -2*(y_true - y_pred)/y_true.shape\n",
    "\n",
    "\n",
    "class CategoricalCrossentropy(Loss):\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return -(y_true*np.log(y_pred)).sum()/y_true.shape\n",
    "\n",
    "    def gradient(self, y_true, y_pred):\n",
    "        #your code here\n",
    "        return -y_true/(y_pred)\n",
    "\n",
    "    def gradient_with_softmax(self, y_true, y_pred):\n",
    "        return y_pred - y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGQVm0CdER_A"
   },
   "source": [
    "Убедитесь в правильности работы ваших функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_FliezhER_B"
   },
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}.\\n\\ty_true:{},\\n\\ty_pred:{},\\n\\tactual output:{}\\n\\tdesired output:{}\"\n",
    "\n",
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    for i, (actual_value, desired_value) in enumerate(zip(actual_values, desired_values)):\n",
    "        msg = ERROR_MSG.format(i, Y_TRUE[i], Y_PRED[i], actual_value, desired_value)\n",
    "        np.testing.assert_almost_equal(actual_value, desired_value, err_msg=msg, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxG76DxAER_J"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "\n",
    "SHAPE = (10, 5)\n",
    "Y_TRUE = np.rint(np.random.random(size=SHAPE))\n",
    "Y_PRED = np.random.random(size=SHAPE)\n",
    "\n",
    "MSE_ERRORS = decode_answer(\n",
    "    b'I0+rUDZfsj+65BmGzuvTP+ITdyZtp9E/jovoXZbOxj+tM4rNGfjRP8WqZPINQ7o/qkWFqWJrvj/YygOu3+7iP2xXHf8FLtc/nh8u'\n",
    "    b'JeH90z8=', shape=SHAPE[0]\n",
    ")\n",
    "\n",
    "MSE_GRADIENTS = decode_answer(\n",
    "    b'bbbcKhRtpD9tOdGaZi27v2ZYmkJFG3s//n322ZiNyr8m6+LE9aWjP6AI7iZLdsk/C3BaoCJ51T/NU4CxsQalvyCAv50cwdI/Jnla'\n",
    "    b'YuW0uL+ge/FUpbTMP8oHwWfBI8g/078UJnJrrD+agsQdii7Xv/P1Wpvcppe/ALOZhal2qz/J0z1DWiXQPy69kl9AdMK/KyUC2VwB'\n",
    "    b'zb+TKsfV3+qvP61ZjQQ/I6y/861aiGl80T9aw6ZZnY2hP05TbFQuN8a/pf7fNLXD1b91mBtGF3LIP2BQLEj8766/QK1QkZqzrD8A'\n",
    "    b'Z3EKfd/APyILN4pDvMK/IkWU2OF8w7/dKXoh5znEP2Op3oXyxcC/bf/5e0B1s7+ti6ZZedDDP8APmQhD5Ms/m5mMtC1R2T9T4D8Z'\n",
    "    b'LzfZPy+LSRFEHtC/gzVVrZfSzD+G39szoJO5P2acoFCBSoO/d+LLLFDn1D+yZblH9XbYv5OyW97C+8Q/nCtIKLzV2L9dGTmp/Em1'\n",
    "    b'P7SuSAYBJtA//Z/XUtyLwb+9hvZzq4e5Pw==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTROPY_ERRORS = decode_answer(\n",
    "    b'Lk/El56cyj/WBoxO/q6zP0unis259t4/pUfrxKNr0D96LACShLzgPwpVpmwNtr8/yNI87LnRyz93hYk3LG3JP09lbzfFF+Q/stF9'\n",
    "    b'Vtkm6T8=', shape=(SHAPE[0])\n",
    ")\n",
    "CROSS_ENTROPY_GRADIENTS = decode_answer(\n",
    "    b'AAAAAAAAAICQTFUt2sf1vwAAAAAAAACAmWpZOGWeAMAAAAAAAAAAgAAAAAAAAACAAAAAAAAAAICCQnznotTxvwAAAAAAAACAXyHI'\n",
    "    b'8ogW9b8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAID9kuL7NywlwN9kei0D+/C/AAAAAAAAAIAAAAAAAAAAgNm59BtSBPm/wPXRt3J0'\n",
    "    b'AsAAAAAAAAAAgDhJMLFijPK/AAAAAAAAAIAAAAAAAAAAgIi4e0BwQ/y/9c0zp6WyGsAAAAAAAAAAgHD+3KnZ2PK/AAAAAAAAAIAA'\n",
    "    b'AAAAAAAAgFqtaQ7QO/m/QoDlrBTV+b8AAAAAAAAAgAhQB8ahy/e/DnZxuenA878AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAgHRxm/JtmQXAAAAAAAAAAIAAAAAAAAAAgKLsIAbIYvC/AAAAAAAAAIDiZmdoeIw2wAAAAAAAAACA5l+CC9q6QMAAAAAAAAAA'\n",
    "    b'gAAAAAAAAACAtRNb8pFX+L8AAAAAAAAAgA==', shape=SHAPE\n",
    ")\n",
    "CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX = decode_answer(\n",
    "    b'COSTNVmIuT/kw8IgYPzQv0B3oEkL8ZA/vw46iH+Y4L/wpRs2c4+4P8iKqfDd098/DgxxSGvX6j/AaOAdXki6vyhgL8Vjcec/cBfx'\n",
    "    b'uh7izr9E7RZV5/DhP7xJscGxLN4/5PfMVyfDwT9BozWlLPrsv3CzMcKTkK2/4A+A8ykqwT+7SA3UsC7kP3psd3dQEde/O1ehB9og'\n",
    "    b'4r+cepzly/LDPwxY2GIHlsG/cFlx6oPb5T8wdBCwBPG1PyJoh+n5xNu/Tv4XgqI067+SfqIXnY7ePzyyG639VcO/SGzSmkDwwT/A'\n",
    "    b'wA1NXBfVP+rNxGxUa9e/ala5Thpc2L9UtNjpYEjZP7xTVidv99S/SH/4mpBSyL+YLhCwl8TYP9ipX+WpbuE/AsCvIXml7z9o2I/f'\n",
    "    b'+oTvP/vtmxXVJeS/ckFVzJ4D4j9o19JAiPjPP4DDyKQhHZi/Fdv+NyQh6j8ev6eZspTuvzif8pWzOto/gzZaMisL77+0X4fTe5zK'\n",
    "    b'P2Ha2keBL+Q//IeNZ9Pu1b9sKPRQlunPPw==', shape=SHAPE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rikEpNDaER_P"
   },
   "outputs": [],
   "source": [
    "mse_errors = [MeanSquaredError()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_ERRORS, mse_errors)\n",
    "\n",
    "mse_gradients = [MeanSquaredError().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(MSE_GRADIENTS, mse_gradients)\n",
    "\n",
    "cross_entropy_errors = [CategoricalCrossentropy()(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_ERRORS, cross_entropy_errors)\n",
    "\n",
    "cross_entropy_gradients = [CategoricalCrossentropy().gradient(y_true, y_pred) for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTROPY_GRADIENTS, cross_entropy_gradients)\n",
    "\n",
    "cross_entropy_grad_softmax = [CategoricalCrossentropy().gradient_with_softmax(y_true, y_pred) \n",
    "                              for y_true, y_pred in zip(Y_TRUE, Y_PRED)]\n",
    "check_answers(CROSS_ENTRY_GRADIENTS_WITH_SOFTMAX, cross_entropy_grad_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t5KvK14PER_Y"
   },
   "source": [
    "**Задание 2.** Выполнение прямого и обратного прохода нейросети\n",
    "\n",
    "В этой части задания вам будет необходимо реализовать forward и backward проходы для 3-х типов слоев:\n",
    "* `Linear`, выполняет линейную комбинацию входов с весами $y_j=\\sum_{i=0}^{N}{w_{ij}x_i}, j=\\overline{1 {\\ldotp \\ldotp}M}$\n",
    "* `ReLu`, нелинейная активация $y_j^{(n)}=\\max(x_j), j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "* `Softmax`, $y_j = \\frac{e^{x_j}}{\\sum_{i=0}^{d}{e^{x_i}}}, j=\\overline{1{\\ldotp \\ldotp}M}$\n",
    "\n",
    "Вам дан шаблон нейросетевой модели, которая умееет последовательно добавлять слои друг за другом. Сейчас вам стоит обратит внимание только на функции `__init__`, `add`, `forward` и `backward`. Пока при инициализации модели параметр `optimizer` оставляйте равным None. Также вам дан абстракный класс `Layer`, который предоставляет интерфейс одного слоя нейросети. Вам необходимо реализовать функции `_forward` и `_backward`. Все вычисления необходимо делать матрично.\n",
    "\n",
    "Замечание:\n",
    "    обратите внимание на функцию `_build` слоя `Linear`, веса соответвущие bias'ам добавляются последним столбцом.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIfc4ZZ6ER_Y"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, loss=None, optimizer=None):\n",
    "        self._layers = []\n",
    "        self._loss = loss\n",
    "        self._optimizer = optimizer\n",
    "        self._outputs = None\n",
    "        self._accuracy = 0.0\n",
    "\n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not self._layers:\n",
    "            layer.build(optimizer=self._optimizer)\n",
    "        else:\n",
    "            layer.build(self._layers[-1], optimizer=self._optimizer)\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = inputs\n",
    "        for layer in self._layers:\n",
    "            outputs = layer.forward(outputs)\n",
    "        self._outputs = outputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backward(self, outputs, use_gradient_softmax_with_loss=False):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "\n",
    "        if use_gradient_softmax_with_loss:\n",
    "            grad_outputs = [self._loss.gradient_with_softmax(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers[:-1]\n",
    "        else:\n",
    "            grad_outputs = [self._loss.gradient(outputs[i], self._outputs[i])\n",
    "                               for i in range(outputs.shape[0])]\n",
    "            backward_layers = self._layers\n",
    "\n",
    "        grad_outputs = np.array(grad_outputs)\n",
    "        for layer in backward_layers[::-1]:\n",
    "            grad_outputs = layer.backward(grad_outputs)\n",
    "\n",
    "    def update_weights(self, x_batch, y_batch, use_gradient_softmax_with_loss=False):\n",
    "        if self._optimizer is None:\n",
    "            raise ValueError(\"Optimizer is not defined\")\n",
    "        self.forward(x_batch)\n",
    "        self.backward(y_batch, use_gradient_softmax_with_loss)\n",
    "        for layer in self._layers[::-1]:\n",
    "            layer.update_weights()\n",
    "\n",
    "    def fit(self, X, Y, batch_size, epochs, shuffle=True, X_val=None, Y_val=None, use_gradient_softmax_with_loss=False):\n",
    "        size = X.shape[0]\n",
    "        X_train, y_train = X[:], Y[:]\n",
    "\n",
    "        self.loss_train_history = []\n",
    "        self.loss_val_history = []\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            if shuffle:\n",
    "                p = np.random.permutation(size)\n",
    "                X_train, y_train = X[p], Y[p]\n",
    "            for step in range(size // batch_size):\n",
    "                ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "                self.update_weights(X_train[ind_slice], y_train[ind_slice], use_gradient_softmax_with_loss)\n",
    "            train_loss = self.evaluate(X_train, y_train, batch_size)\n",
    "\n",
    "            if (X_val is not None) and (Y_val is not None):\n",
    "                val_loss = self.evaluate(X_val, Y_val, batch_size)\n",
    "                self.loss_val_history.append(val_loss)\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                #print(\"Epoch: {:d}, train loss: {:f}, val loss: {:f}\".format(epoch, train_loss, val_loss))\n",
    "                print(\"Epoch:\", epoch, \"train loss:\", train_loss, \"val loss:\", val_loss)\n",
    "            else:\n",
    "                self.loss_train_history.append(train_loss)\n",
    "                #print(\"Epoch: {:d}, train loss: {:f}\".format(epoch, train_loss))\n",
    "                print(\"Epoch:\", epoch, \"train loss:\", train_loss)\n",
    "\n",
    "    def evaluate(self, X, Y, batch_size):\n",
    "        if self._loss is None:\n",
    "            raise ValueError(\"Loss is not defined\")\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            raise ValueError(\"X and Y must have equal size\")\n",
    "        \n",
    "        Y_pred = np.empty(Y.shape)\n",
    "        size = X.shape[0]\n",
    "        self._accuracy = 0.0 \n",
    "        for step in range(size // batch_size + 1):\n",
    "            ind_slice = slice(step * batch_size, (step + 1) * batch_size)\n",
    "            Y_pred[ind_slice] = self.forward(X[ind_slice])\n",
    "        losses = [self._loss(Y[i], Y_pred[i]) for i in range(size)]\n",
    "        for i in range(size):\n",
    "            if np.argmax(Y_pred[i]) == np.argmax(Y[i]):\n",
    "                self._accuracy += 1\n",
    "        self._accuracy = self._accuracy/size \n",
    "        return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAfM9DNNER_d"
   },
   "outputs": [],
   "source": [
    "class HeInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        n = shape[0]\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0/n)\n",
    "\n",
    "\n",
    "class ZerosInitializer(object):\n",
    "    def __call__(self, shape):\n",
    "        return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3sD8zoBSER_h"
   },
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    def __init__(self, input_dim=None):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = None\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.d_inputs = None\n",
    "        self.d_outputs = None\n",
    "        self._optimizer = None\n",
    "\n",
    "        self._is_build = False\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        self._optimizer = copy.deepcopy(optimizer)\n",
    "        if prev_layer is not None:\n",
    "            self.input_dim = prev_layer.output_dim\n",
    "        elif self.input_dim is None:\n",
    "            raise ValueError('Input dimension is not determine.'\n",
    "                             'If this first layer, please, use param \"input_dim\"')\n",
    "        self._is_build = True\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if not self._is_build:\n",
    "            raise ValueError(\"Layer is not build\")\n",
    "        if inputs.shape[1:] != (self.input_dim,):\n",
    "            raise ValueError(\"Input shape is not correct\")\n",
    "        return self._forward(inputs)\n",
    "\n",
    "    def backward(self, grad_outputs):\n",
    "        if self.inputs is None:\n",
    "            raise ValueError(\"Forward pass is not performed\")\n",
    "        return self._backward(grad_outputs)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: np.array((n, d)), input values, n - batch size, d - number input features\n",
    "        ---\n",
    "        output: np.array((n, c)), output values, n - batch size, c - number output features\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _backward(self, grad_outputs):\n",
    "        \"\"\"\n",
    "        grad_outputs: np.array((n, c)), gradient by outputs,\n",
    "                      n - batch size, c - number output features of this layer\n",
    "        ---\n",
    "        output: np.array((n, d)), gradient by inputs,\n",
    "                n - batch size,  c - number input features of this layer\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rXeRWVDER_p"
   },
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, units, input_dim=None,\n",
    "                 weights_initializer=None, bias_initializer=None):\n",
    "        super().__init__(input_dim)\n",
    "        self.output_dim = units\n",
    "\n",
    "        self.weights = None\n",
    "        self.mean_d_weights = None # mean value gradient weights by batch\n",
    "\n",
    "        self._weights_initializer = weights_initializer if weights_initializer else HeInitializer()\n",
    "        self._bias_initializer = bias_initializer if bias_initializer else ZerosInitializer()\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        weights = self._weights_initializer((self.input_dim, self.output_dim))\n",
    "        bias = self._bias_initializer((1, self.output_dim))\n",
    "        self.weights = np.vstack((weights, bias))\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.dot(inputs, self.weights[:-1])+ self.weights[-1]\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        arr = np.zeros((grad_outputs.shape[0],self.weights.shape[0],self.weights.shape[1]))\n",
    "        self.mean_d_weights = np.zeros(self.weights.shape)\n",
    "        for n in range(grad_outputs.shape[0]):\n",
    "            for i in range(self.weights.shape[0]-1):\n",
    "                for j in range(self.weights.shape[1]):\n",
    "                    arr[n,i,j] = self.inputs[n,i]*grad_outputs[n,j]\n",
    "        self.mean_d_weights = np.mean(arr,axis = 0)\n",
    "        self.mean_d_weights[-1] = np.mean(grad_outputs[:],axis = 0)\n",
    "        return np.dot(grad_outputs,self.weights.T)[:,:-1]\n",
    "        \n",
    "    def update_weights(self):\n",
    "        self.weights = self.weights - 0.001*self.mean_d_weights\n",
    "\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = np.maximum(inputs,0)\n",
    "        return self.inputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        return np.where(self.inputs > 0, grad_outputs, 0)\n",
    "\n",
    "    def update_weights(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, input_dim=None):\n",
    "        super().__init__(input_dim)\n",
    "\n",
    "    def build(self, prev_layer=None, optimizer=None):\n",
    "        super().build(prev_layer, optimizer)\n",
    "        self.output_dim = self.input_dim\n",
    "\n",
    "    def _forward(self, inputs):\n",
    "        self.inputs = np.exp(inputs)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            self.inputs[i] /= (self.inputs)[i].sum()\n",
    "        return self.inputs\n",
    "\n",
    "    def _backward(self, grad_outputs):\n",
    "        output = np.zeros(grad_outputs.shape)\n",
    "        for n in range(grad_outputs.shape[0]):\n",
    "            for i in range(grad_outputs.shape[1]):\n",
    "                output[n,i] += grad_outputs[n,i]*(self.inputs[n,i] - (self.inputs[n,i])**2)\n",
    "                output[n,i] -= (self.inputs[n,i])*((grad_outputs[n]*self.inputs[n]).sum() - grad_outputs[n,i]*self.inputs[n,i])\n",
    "        return output\n",
    "    def update_weights(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAKnD_xeER_x"
   },
   "source": [
    "Убедитесь в правильности реализации вами функций с помощью небольшого числа unit-тестов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BI2DWVi3ER_y"
   },
   "outputs": [],
   "source": [
    "def decode_answer(base64_string, shape):\n",
    "    buf = base64.decodebytes(base64_string)\n",
    "    return np.frombuffer(buf, dtype=np.float).reshape(shape)\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "INPUT_DIM, OUTPUT_DIM = 4, 6\n",
    "BATCH_SIZE = 3\n",
    "COUNT_TESTS = 10\n",
    "\n",
    "INPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "WEIGHTS = np.random.normal(size=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM))\n",
    "LINEAR_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM))\n",
    "RELU_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "SOFTMAX_D_OUTPUTS = np.random.normal(size=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM))\n",
    "\n",
    "LINEAR_OUTPUTS = decode_answer(\n",
    "    b'wGm3fs21079DBbbsam/fv0YsX4hX9/o/Nxh4K63n8T++UomsDXPrvxbi2hKA8gjAl03DtDo1AcB05j4D6Bvcv1d2xALzbwBADAx/'\n",
    "    b'H0G0AkD31QxMgaX0P+gv0xPBnfK//f64pm5VBUDLFwHUBV4EwAiYgNzWLA3Ax4deeVK9AsDYJpe26kwEQGVn2t7ZWwLA/HC9xMss'\n",
    "    b'9L+IeOKWVevZv5Rfd295DbI/WHvyPVAW979E6GKRpFPmP8x4zXNcUwTAqCRqOrLM8T/151+zeYr+vx6bEJiWGNC/mhc6p5SqAMCh'\n",
    "    b'SuMGWLDcv24FuPkfPeU//mbyah5d9r+ucwIVIU3oP81dEoOegQVAyKhCIITQDcAwTKAcRY4NQPjHLQNXwALAestiiX2EBkDemn7g'\n",
    "    b'j8DwvzBqu0YGnPw/wNLLm4gz8r8MoZpd8svwv/70sbu0xP2/4Z0Nh94+AMAow1b45oIKwMdHYH/MvwrA506PZHXj4z9aLm5Z2+3s'\n",
    "    b'P8Tk/paM3gZAqCclSm3M+z9pzl5fkHIYQBkKiIdTjRJAkWkBGd358D/IbqQCKN77v7JS+XYenvC/Bo9V1xLnEEAkelfNQC3kPwGY'\n",
    "    b'/nu93/8/sE1Qipm89L/QNoNnBFqkv0TCcC86m+I/tLP+xucZ0j8QA6VJV2HiPyc1Ru5nFgJA5efyWSarA0Cc5Fqc2qrNv2vZQdXk'\n",
    "    b'sQDALOJVE8sv/j/mrk0EYifjP+4ZCcQaJPc/V0d//SUe5b+gPNHw9VzSP/hOIius5gTANiVWAomHFcD7wcLEf3Dwv8EPRV+UIBlA'\n",
    "    b'KHyQtLH1+T9gr+67iXUGQHyz3s0wTNE/1vmK3JcAD8AxuOL3pTL4vwVlfP0a6xdAKk4O0RKRE8DzJk+ETJP1Pwml9+piFgrAsEbz'\n",
    "    b'ANbAC8AYXacpBbXOvxRlJ9bEeOo/yaILuC6LCsDiNBbTRO7Vv6RKEPT8Avu/2uPPVplQ0j+jPgFnam6xvyGX5u0PhPC/X2dnCYQR'\n",
    "    b'EsCgGkfU3kHgvxQ0Am3FNss/V6Y63wHpC8A7tmYislPcPxQ+0M8ZYhFAQIPzNBpF+r9Yv2NY7SoQwOyBsYbZNvI/qLCJjvaL/D9S'\n",
    "    b'O4Mv3CS1vyftMo9OnAvAhP5pHLgYFcAg3GyiFC/pP+R7Q28JJ9C/p96cZCbV8z8pY0LfW6vpv2tGIwTKyxFAmtN1h/cK5T/uR0Ff'\n",
    "    b'LpEEwIOjB57uAfw/iH3ZhwYzB0BipUya/4/pPyz9FjDi5hdARbRzUdzv7j+YYMxaOSILQHANapLprda/IlSg8sQ28D8xDBK2gxnm'\n",
    "    b'P5CW0s+gnbY/gDZeg85Czj9SuLWI8vwLwHZfxhQpCN2/ir6FkxvEEsCShlKitBX2v9rq22/+hvO/QiehO1HX0j/eww3Ft83nP6Vs'\n",
    "    b'dgmGtAZAlP8lO1X99D9QydHZ4nXvPwZiWA6Ssea/TJJIWptA9r/uWjqs06v+v9An3PViyO6/KrQViu7RCcA+Kh7hTMu/v2zoa2ZW'\n",
    "    b'jBXAHrrgqgSM/j8gvM4nNqT3v4xtpLcZYRdAfoxz/N45DsA80sT+DY3yP15FUTkjIPo/oLiF9fzN9T94wKgxQHy9P4DTllkw19O/'\n",
    "    b'MtKPpb+g0b9yPM4rr97iv6xle4TvzP2/0gweWXoCDEBU3w4wKx30P9hmJE5kSAbAJkOcNM34G0BNIxfvXRcTwKb78y1erPe/lJ/Y'\n",
    "    b'redW+L9e47yfGZn1P0ZEwUoRiQHAzIFyAjDGAUC8bw0idVf2vyJwOBRMmAFAL+NuZOhDC8D+5VZNXyoJQLCicm3ysPW/SB7Coj/M'\n",
    "    b'2T8sjM1DuEzrv+rph7ycMhFAPyaGKEi3E8A02vtrHaXqP6L4cDNZe+y/c/Ys2fxu5r8nf4qpwmcSQIi0Q9ysMPI/Fh10yO1sDUAG'\n",
    "    b'6RLr7PUDwG5OmwH+59o/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, OUTPUT_DIM)\n",
    ")\n",
    "LINEAR_D_INPUTS = decode_answer(\n",
    "    b'P2edUgWR+r/bzKvlk3bjP9szpDHiifi/LYE0nMt2779h0J1t7RAMwPm13hRgIvo//0PlFKvn078mY9lXhU7dvzHH2LklFRVAj3BH'\n",
    "    b'TpX7AUDEabx1uEIHwBJTpAf2u9k/2UASQZeD0b8ZiUrHtnD5PzUUC+5tqgHAhKA4PIjR8L98KhExIHv1vyfDZ2OKQPy/z8zIh0tu'\n",
    "    b'DsBRGTYBOEEJwLQRGDDCDRHAq91D5VKlIMD95gLQO6QaQI8ES0rhliJAfZgckVxv9T9G/aTxuLrFvwv4niwXcfg//CNiqtSf67+s'\n",
    "    b'xePaogPqP/8f44x7EwzATjSTlQa21z+fmux6IjwPQGMvrv8zShHAbJQRvmQGAcBPLH4HSQ72vxmsITDMDiNAlDrNEjW99D9csOJ6'\n",
    "    b't7T+P93HoYdoKfM/7k3sVZ6J9r/6iK3y2S7ZP/l1IQvc7QTAFTesB+Nb9L9kIG01nPj/v++DG90/aeA/bOFczA0HAMBKeBXJECcV'\n",
    "    b'QEJNe+3vpBvAnwLL8efe4D+bRDTq7ezfv21YsH6B+vM/L6CZoBR21D8hMDDp4yT8v6XULQufIPY/s7GQkbha+b8exktRol0VQAQM'\n",
    "    b'qBQ/ZeI/qeq5APRYjb+lhC8HfqQCwBQiLbLre/u/0xv/24Gi3r801ygQjJv1P2KwMlh6H/u/NUi9uPw/8z8t3X2eBLfzPxLxpw5f'\n",
    "    b'N+c/RyP/A/ah4j+EobO9K38RwPXhyZcCAQFAVmb/UNjVB8Cd/+qN1o0OwK8yktnHcAjALgt4XYrD+L93CbzwJHv7v/0mhN5VO+u/'\n",
    "    b'SzTIYQ0H4j8Mc01xk1QPQFeOI+NWUQXAsP1mWBWl4r9uhE6QjNMEwGcPimeXce6/yI1g6owq8j9edpTiY4H9v1VBS0rUMwHA3wpn'\n",
    "    b'JKiM+b+gmZUgyMLWP4WXYJ9+lw9A5fhvmrbi5T8tCSJlLwIeQJrtuTutefk/4bkQwFPWEsAgAwRhO+QLwHwagU32TAHA4SMOiYKI'\n",
    "    b'7L+6FngpMMgBQDjYeTf1/Ok/4+2b7LP1+b+gXzZ1qKgMQBElUKRe0tW/cOWiD+H+9T/QarjGuP/8PzmlXoBeww3AIHsffOvw5b9P'\n",
    "    b'4Y9kZCcKwFfPnIHSDwtAwb567vp4wz9AImNVabrQv1ilXYAKTua/yRTFuDpc0b8Rxpcdo6QLQFhZ/XLpVQfAzrFbKSnx8b/4OfW8'\n",
    "    b'ovQFwL295MDX1Oo/pTQXX+ZsAMAKax04xwvwv8jcCidyEgLAYlrJn2I4CUBWSMjMKagOwAkewz9Jnum/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "LINEAR_MEAD_D_WEIGHTS = decode_answer(\n",
    "    b'arfqHIAf479QsLlR0m7Wv+jN7ldX6+y/BpChF3zO4r+iO2bPKy/kv/2bdU6oisS/DiFD39Ma1L99QrcdZLrNv0KzC2HEb/K/n35H'\n",
    "    b'WJ2J3b83YGFa7N7kPyNAC1DDoqS/+CC4vvv+zj9Das+5yKinv5tyGRSeGMI/OWWympzx07+Ftb6M3CfgPxMqUKMHYOk/Nb1MLbVm'\n",
    "    b'6D+MCqeZckvXP0VSDrTOPvM/QNo7F1dP4D9TTSLALTPmPxnZ6L6n8uE/3Eoyzo780r+fv7SODo61v9CWDJI137k/4DEvqKkblb+7'\n",
    "    b'URIg42vxv2kD6okB3NS/1AllJjKQ6z98C8dIGr95P81bRcd3OMa/BFQe+lkr4L+/6OnoQdOjPzF9zjIkkK4/FIdNlRa5yb8VCkvn'\n",
    "    b'xhHBv9Uf8azEtL8/ynoNTAMDoj9/VXgneSTDv6jl+qMRrsq/d2DyQa0S+j8ZYBX9Kd3Wv3PPiSI6+bC/hABDe4iF67+1RrhSNQnX'\n",
    "    b'v4wdMNwj5tW/7ADbBBAx47+VPRR0msPVPzEC2dIDlrO/S3U26I2L6T9w8uvHkfbTP9VwiTbq7+A/J46sCQnu8L8r2SLRrSZ8v26W'\n",
    "    b'ATJ6fMM/i2TxxobCsL8doERHZ/aiP8NirrEgmtC/Y/YOGLrk5b+tous4Fn/Tv4hZTNPnst6/QTwB4nVetb+/ehcpxL3HP/EEdNKq'\n",
    "    b'lKs/aCawyDkg2T9VvYpYnG6nvw9Oou01DM0/B26MkVIuwj9VcuoOy1/wv+aK6ClOqdS/GZdYPeuC7L9PJw3O2YPnv0yt1binSPW/'\n",
    "    b'AvgQvByj47/p2QORQK/bP5kxgHJOquq/Vdq8oh/F6j+j8vUqhWvaPzIRwuMfwOA/DCCeqHZohj99+R1SI2S/PwoU1KtTfLC/YGIZ'\n",
    "    b'rKge77/DRA+HjlO3v9P2TkRFW9O/GBq7h6bRqD9b951ij0jtP0vtuEvb9uc/EXFrNq2E0D+D5QuRJ7LaP4OWI2iSLtK/8VhmW9yj'\n",
    "    b'6z+dFaRn5u7YP/PEW9U1tdy/bPa3t4Bz078x2BNPJKPRv4tp66+QyMc/SVuo3FgN1L8J3jF1nFGaP3TnWOkLt+c/2p2TlN2b5L8o'\n",
    "    b'wi8r7ofUvxs0iRHaQuQ/91Qlr9zt5L8MdWywPQGvv7Qm8jXTfec/Yz3Vr3y41r9ZP+2CQdbEv8rYDpBmOqA/l1NCINeT0z8JH8QE'\n",
    "    b'xXXgP76jPbRvg/M/oSJ5KdU81D8VQK3fVCyeP2vMKOhM56U/BZzrQA065L+Y7tDchoDmv0octzYo2PK/09Tl2se20L+VR9p6w6ng'\n",
    "    b'Py97+8+YdO8/GY2Ne+aDxT//8cj2z4qnvyWuZEad4fU//Jk/MSbXxL+8SsKLs3LgPxx0RhYjCu8/rOc0IsdnuT9Il9BhEieov3dg'\n",
    "    b'rP6KYPM/32QVYgu+0z+tURTx01/evxOE+J83UOm/SOaIvyUquL98UvdfsyO1P3Djfkny2/a/b+oz3DbMwb/U+UYpwcjVP3wfVR7E'\n",
    "    b'0Hk/odno8Hwq4r9XLF9XhDrSv6mFfNKLPPU/8HTl7HDNqr8P80KavPfePzXrvspu5+k/cQNiV0l3tb+biTHuj2u2v/MKMn9aIPE/'\n",
    "    b'i4Sjw58d5z8JlSifqwagv3cTV0MnJsm/zwNNPCVt8z/4vZrAbNHVP+gMHM9KvNo/KaoVfEMGAcCPuG9bwtziv8QXBlKdu+4/QzGT'\n",
    "    b'GegB+b9lyoyc+1XoP+8L5L0RquO/ud2z2b/s8D8eEsYVvIjiP1vPVU+/Cey/vFGN6Z9Y5r9vdHeb1az1v9H53gnWF72/gAvQjsoI'\n",
    "    b'AMCPnUx6VQnnv+NlhZ7DnPE/NC7dg5i/5r/HnktUju3zP3f4u/xaCde//W+EBL/C8D9h+aILAbbgP0OyioqCqey/WWjYgB0R5r9z'\n",
    "    b'c30eDzz0vwTjxye+sbq/ShLZt0cwsz8ovJGv+SHkP4Vl9csAA7C/rpcmJq4Z5b+EfET8lCvHP26w3mw9f4S/M9Cb/myByr+XP+IA'\n",
    "    b'c4XevwmdPXKelpo/3aoZilJwzj/5E1EgvB7Avwkbzzl1q6u/G4Wj3MoP5T+FnN3skvfmP8XZHeUb/ZM/59vxTkuRwb8jL7TU7aTR'\n",
    "    b'P1XrcMOdsrk/aWPI3zX0tb+3s6jTXT3Yv9mqBKsh2K0/uJXE+4YR6b9MJvsDFPbLP/Gx8J+Vdte/mX7Xh41b3T/D+uSIDyTlv4DA'\n",
    "    b'qy2aRcQ/tdjMwp6p7j/ldmmKBnqhvxVipc8j1Y8/kY6ZUT/82j90Z6cf+vKfv/0jJMafAtg/t8+tW7Tbvj+454FNAAimv1kdWjA2'\n",
    "    b'XOi/CzZ2TdEo279o8RYchjq/P0OZtGmgxtO/JezwgRUfyL83blLkT2yhP/knW//60uo/1fEmQXrO5b/2+0nprcHRv3vLkB5zzt6/'\n",
    "    b'19DiGucd3T/M79EZ2qXmv63cP7s/kdU/MSsrLva51b+MhISLNzjdv7Kw9U8d0eC/geJJfPp72j/zlqT5CIXMv7fb8xOe1IO/bkbV'\n",
    "    b'wBEO0T8lJPnG377gv+o7CHnhleC/1Qhtj/ZJpT+IJwjf7srqPwG3ZFVGus+/H3VAagNd1L/AxlnIbfr2vzYGDaCWLtK/B5QBlpJZ'\n",
    "    b'9b/BhKXLoaXjv/x0heWipOi/Sem05AlN5b/fK/5MeT7ev/XH6nI1a9K/28QbyMIu4r+cuOK2SWXyvwSSD7LmPcm/4cdGYiw62r80'\n",
    "    b'ZFYQs/0DwKmgrBRly9a/4J3ru6g3BcAcQAUQu8b2v2C1VwG7Pue/7swLWfwq0z+rJCsbpIPNv0MB99PE9sY/P2TEcWzp4b/oRKlM'\n",
    "    b'/JXKv5+O0sNMe+I/qz3oRYwh6L94gghIDufvv8xcrZb619i/nABHslxf8L/bB/6O4I30v+t/YsOD0uC/bF58i6Iv3L8B+DE7Wsb3'\n",
    "    b'P1/XDYzjKOO/CvVrQwC/4b+zkB8mNBndP6N8te73c4w/LaByY+YuvD++sT3s1H7Qvzz04N5Lesu/YnFoVUVdkr85E4jMjraXvxGb'\n",
    "    b'kuucnJq/dUkB69gLtj9fm91p0qDVvzQrCT8+oNM/V7k2cw/vuz/PbjpBIAi3v8mUtCcw/dK/O6P/YvXzxD9yZgOdPh3iv34KQUqg'\n",
    "    b'adM/atEm4hTJxD9EZC3renrCv4JwkSmHAtO/X2JvdLjZ4j+RROs4lg38v30HXm6IBck/a3HT1JvmxT9tLRZWofnRv6x/rrVosvO/',\n",
    "    shape=(COUNT_TESTS, INPUT_DIM + 1, OUTPUT_DIM)\n",
    ")\n",
    "RELU_OUTPUTS = decode_answer(\n",
    "    b'YiyQmO8F3j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACg2CiUz2TzPwAAAAAAAAAA5CtrTHaEvj8AAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAADJCr78bJvE/WS8D2voW5z8AAAAAAAAAAAAAAAAAAAAAaQxgASdm0T8AAAAAAAAAADv4UNwHJeI/V/k5EMmt'\n",
    "    b'0T8AAAAAAAAAAAAAAAAAAAAAIy7b6g8YvT8AAAAAAAAAAKTzoe+yzOA/ISOwt7Dm2T81IbokKXfiPwAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAACSemmQTAnrPy1VcWBaNvE/AAAAAAAAAACF50HKCEz6PwAAAAAAAAAAwd68x2zZ1j8AAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6952Oveto/vCtLVNO00T8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAAXxHvVLep7D85r0oej8TpPwAAAAAAAAAAt63DYXGGBEDB+lOtbOb2P/bXRrHncfU/AAAAAAAAAAAAAAAAAAAAAJBN'\n",
    "    b'ZdUcS9o/pBgdwQ8L6j8av8TZeuXAPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHkBh1wAFfI/AAAAAAAAAAAAAAAAAAAAAAaRztEK'\n",
    "    b'uvk/2udSvApj8D/gzG3RNDriPwujSd1sB+w/AAAAAAAAAACoQyp30y7vPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABgTDylweOE/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAAzBuD87oug/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEltIyyUPmP2Oqm8b53tU/iBCS'\n",
    "    b'chK27j8AAAAAAAAAAAAAAAAAAAAAtdJt8fMqwz8AAAAAAAAAAIi+ZPDzAeY/DW95bLqVxj/pR+S/0s/ZPwAAAAAAAAAAPraYpdBN'\n",
    "    b'0z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVSfL4M2f3PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHGHHIgYB/U/9MSA1TkZ5j+m'\n",
    "    b'K3z4Rd3vP33JHTebLANAKp/kCql0jj8w2YC2AtwKQAAAAAAAAAAAAAAAAAAAAACmls8Rbq3sPwAAAAAAAAAAAAAAAAAAAAAAAAAA'\n",
    "    b'AAAAACSn9qtmWNg/oXMhsVm4tT9KDGAkRqzbPwY2K17MUfg/AAAAAAAAAABnNImjpzTjP5ENrHP7jNE/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "RELU_D_INPUTS = decode_answer(\n",
    "    b'A2itoRO18L8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAIAHh/vZZPO/PwAAAAAAAACAWRIg0o5y1b8AAAAAAAAAgAAAAAAAAACAAAAA'\n",
    "    b'AAAAAAAAAAAAAAAAABE1RhH4o8o/KgQl1O5e5L8AAAAAAAAAAAAAAAAAAACAuuBfgdfy3T8AAAAAAAAAAOFo1PooyfQ/kqsXtUZd'\n",
    "    b'7z8AAAAAAAAAgAAAAAAAAACA0Ev2eU0CzD8AAAAAAAAAgDOyhn98d+k/RvVD7zz+sj8PYlvxi/bPvwAAAAAAAAAAAAAAAAAAAIAA'\n",
    "    b'AAAAAAAAAAAAAAAAAACAAAAAAAAAAAAS5p+ceXSxP4Kfa3xtPuk/AAAAAAAAAICRu8tJJc/tPwAAAAAAAACAVNcnv2uq8r8AAAAA'\n",
    "    b'AAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFqConRSR+M/qWsKZIi7+b8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAAAAAAAA'\n",
    "    b'AAAAAAAAAAAARSt0hcOH1L+esNsN7Yn+PwAAAAAAAACAez+0wK/A3j8Itz0J1tjvv/ivP2za98a/AAAAAAAAAIAAAAAAAAAAAMVB'\n",
    "    b'rpc6rOk/hzNsPsw04D/jrFsZmwX6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgFLdAtr9fvi/AAAAAAAAAIAAAAAAAAAAAJkqln2f'\n",
    "    b'2t0/IChXD6IWwL+9Ou0Wx5z1Pz3sQ2fQrvO/AAAAAAAAAIDiIBDEYqMCQAAAAAAAAACAAAAAAAAAAIAAAAAAAAAAANEfbPk1BKY/'\n",
    "    b'AAAAAAAAAAAAAAAAAAAAAEvJuYm5Gt4/AAAAAAAAAIAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAB9zMZQP/PJP8BXEKpVDag/O7Yp'\n",
    "    b'MAiSwD8AAAAAAAAAAAAAAAAAAACApRbFTUOw6b8AAAAAAAAAAAG3xBTkt+y/vEFvt/tC/T+ejuHmq+/jvwAAAAAAAAAAKMW0BHXh'\n",
    "    b'1T8AAAAAAAAAgAAAAAAAAACAAAAAAAAAAICXmOxlXy7nPwAAAAAAAACAAAAAAAAAAAAAAAAAAAAAgPAWWwNPlIS/i9lVufSUz78O'\n",
    "    b'xmPOGt72v8/IkgrUPvS/3AyaWWXA5j9JW3lZsi7qvwAAAAAAAAAAAAAAAAAAAADWcSkNqkPUvwAAAAAAAACAAAAAAAAAAIAAAAAA'\n",
    "    b'AAAAAC6UQFNDQOy/SSAyS6fH/b8/A8h1B8Lnv1G0bA9o/v6/AAAAAAAAAIBUspOra+zrP+v3rx0Rmvi/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_OUTPUTS = decode_answer(\n",
    "    b'xXhAbjms4T+kt8Giv6nQPzVFkyFmjrM/DBZi4c9ovD+9/2O0t+7iP44M7/No88I/U3O850ljyT+nBBJLubmvPxT87vyvibo/okfs'\n",
    "    b'spSinj/Z0qx5mibDP4qJX7wO8OY/l5LFH5o63z9Thj+aifG9P8BtGLOBd7U/ZXDkDCPr0z/IwiZOFI3EP5s57yGFtds/TWOx06O3'\n",
    "    b'1D/UBjCNMzG1PygFI9W5Y8I/d2S+q7Q01D+XoIyKpHKwP83wDEfFfN4/TZGKROcn2T/Z7S4nI+PdPxWaCFi/Kag/ZbYVpfa+tz/j'\n",
    "    b'y3AyoZfIPwqapYg9YrY/+uXC6quTsj8GvfaEWrvkP7TEueeHU9Q/U4IzYvzcuD9ssTWdee7hPxF+31RchJk/+H6EohVj4j+p1hFy'\n",
    "    b'9jbKP2yoaJx9abE/RtmnNfSHwz+EAnm6CdCyP3NM4EVc9dk/Phc5Ih2i1j9dt5BSCGnFP/e/vb1UidQ/etZ55g/8zD+ImpUDiLLU'\n",
    "    b'P4x035Y2jMA/EQnOG8JrwT82R0KU99O/P+R3sWe3B5E/FqlGy1Ii5z/OMhEaAWPdP0XDudD/1No/itb+kyNvoT9zvNSKami2PwTx'\n",
    "    b'EuhvENI/Gp3wsa8H2z/usO6aUlbLP6RlFGLc8rQ/puBG0pL5yT+xPT3zjBmxP2jJA2Dbs+U/oWst0OSoqj+hqzDFonKZP4UIrPFZ'\n",
    "    b'UuA/WIhcsW810j/Gt/B9ZB3HP5yWGOW8Bd0/WroVVtIvlT/YhpTupwPgP7UA0IVh/5k/orWs6WSIxz+sf3qzlWPQP3KcH9Yqm94/'\n",
    "    b'TiQ+BjT0tD/0gvjjYly3PyWmjDoWgeU/RA5ViJ4ftj/SnqZfpj3DP16af68gyMU/6Wf0OUhMxj/U69O12KfYPwkTctXyTdE/IWc2'\n",
    "    b'XM0B4j9ShSGuPi2yP3R+u6ZNrr0/z/BbMoIF0D+4D+jmodq3Pwm+Bal1rNg/IvxNqzOYzT/5P5kHyJDSP8mZuU3jR9U/d73rJi3M'\n",
    "    b'4D8+PRtUOHmmP4IOrmdtQrk/wpkNSXWMsj/UJG9Abf7pP8SFlAqk2qA/vfwurs4StT8zl5ubCdmzPyHFqQjqFdo/duHz1n72yz9X'\n",
    "    b'ZPUklPjSP8wdLsKQ99A/YYHty7gTmT/vJlfHtCvmP/PDSyai/JE/67D3fQJksT8qxg5hUlbiP3Ws7SgtYsI/amLbE4iSyz9NPV2T'\n",
    "    b'XXKbP4+nZp+yQ9Y/s4VBEkSR0D/p/iF143PXP4B2/H6DleE/eUUAMLjHsj+sRXJKPgnMP5q9m6HXPMQ/',\n",
    "    shape=(COUNT_TESTS, BATCH_SIZE, INPUT_DIM)\n",
    ")\n",
    "SOFTMAX_D_INPUTS = decode_answer(\n",
    "    b'WIS5wPxs4T/tYtAttQTgv9cCAzg36bG/Et7nguqbmj+RCJAVjLXIPzxW5KylgX2/ipaHs5Vswr9MPYXSpHOlvw2PEG2zW7y/JwA4'\n",
    "    b'6NU4ab+dTU/lBrmwP+ACBm7m2Kg/B/NST+5g3r9C2RHZsTzIP/yg5DkmurI/T7yhqBcoyz9vVDiNFadzPwpePGQmasi/iCAwIL87'\n",
    "    b'wT+C64leukSqPyixnj8Xe8I/k4hfLUpN0j9E/V2gBuGyPz0wo7qLIeC/r/IQwL1l3D/X0+Wf2/zhvxO113eJf7E/0j0mDrmgqT8m'\n",
    "    b'x0KV81DSP+DxONaMWbA/16nv9zr3tT8U7syIJeXbv6+7H1CZlbW/+coU9Og+vz+ZgoyXbui0v5nmLuc9fqY/dXkJ2kNvxL9McPuv'\n",
    "    b'eP7UP9BQ3+dJ66s/WTvl/3+IzL/IA9At1d6yv0xMvOWEcMo/2sbh6jB90b9LQ++GR/nBP/9ZUvw3m9s/GAO8VhNR3r/POg6w6VjA'\n",
    "    b'Pz3RdfZl2rW/qtEOh/cbuj+msDlAwpKyv2ydqxZGD6K/ANsKSNzmdz+QFp++o3vTPzcOtL/SJ9a/OQROdZy5ir9EPvsl3w+sP/ib'\n",
    "    b'Zs5/AsI/B6RNRqRM1D8Q8otoA47cv17+n54Y/Ie/rhdKMI9awT9+ILwcdYOuv8SymrvdwLi/aE6Spec1lT/4QPSzqMCevxRLlmqf'\n",
    "    b'Hsw/NqirnEao0b/0Gn/KBRS2Pz0YL5M80dO/jlzx7lGWgr/hgPyMrDDTP9wjolopVJM/NsC9m/sZur+zRdRP4eW3PypKj8jrO4+/'\n",
    "    b'Go/tE19umD9VAUaMLreoP73ULAeK0bw/jxu5zPBKsz99eAQNCTzOv2omI7+oHtW/OOU1Jclfx79YIGD5GNykP/oUkjIKM94/dtv2'\n",
    "    b'Bq3v2z+NWtKJvHm0v355k/lF6sW/MBBxzzW4x78PMyqAB/iYP/TdmjfU6am/gKn04JArmT+A9W3hAAFbPzf5e97n9bG/aDwuGZW5'\n",
    "    b'vL/eZWZDPuORv4vnQUQmlMk/sNL224W1mD9wTPCp6MeJPwyOtgLSt7c/IiZ5RxiPwL+Rrgnnpyinv3MTNHvFG8M/iGt/imN+nD8/'\n",
    "    b'lcHyZ+HAv31Hy8mFneG/gyw+x8uBkL80oAPJPw7jP1jqzB90lZ2/04p+0+MRnD/qQP8J1fvFv20rlcJDLJy//hTiByH/xT/qnEBS'\n",
    "    b'KCJ/P/B+GpViZ5C/eIHg7ViN07/0Jul1RhfUP8KBvfGu5bS/tIGJ1Kx4eD/V5GOKk927P2T23cve/qC/',\n",
    "    shape=(COUNT_TESTS,  BATCH_SIZE, INPUT_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DLdx0n2ER_0"
   },
   "outputs": [],
   "source": [
    "ERROR_MSG = \"Error in test {}:\\ntrue values:{},\\ndesired_values:{}\"\n",
    "\n",
    "def check_answers(actual_values, desired_values, msg=''):\n",
    "    np.testing.assert_almost_equal(actual_values, desired_values, err_msg=msg, verbose=False)\n",
    "\n",
    "def test_forward(layer, true_outputs, set_weights=False, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, true_output) in enumerate(zip(INPUTS, true_outputs)):\n",
    "        if set_weights:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        desired_output = layer.forward(input)\n",
    "        msg = ERROR_MSG.format(i, true_output, desired_output)\n",
    "        check_answers(true_output, desired_output, msg)\n",
    "        \n",
    "def test_backward(layer, true_d_inputs, d_outputs, true_mean_d_weights=None, **kwargs):\n",
    "    layer.build()\n",
    "    for i, (input, d_output, true_d_input) in enumerate(zip(INPUTS, d_outputs, true_d_inputs)):\n",
    "        if true_mean_d_weights is not None:\n",
    "            layer.weights = WEIGHTS[i]\n",
    "        layer.forward(input)\n",
    "        desired_d_input = layer.backward(d_output)\n",
    "        check_answers(true_d_input, desired_d_input, \n",
    "                      msg=ERROR_MSG.format(i, true_d_input, desired_d_input))\n",
    "        if true_mean_d_weights is not None:\n",
    "            check_answers(true_mean_d_weights[i], layer.mean_d_weights, \n",
    "                          msg=ERROR_MSG.format(i, true_mean_d_weights[i], layer.mean_d_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXb4TnEqER_3"
   },
   "outputs": [],
   "source": [
    "test_forward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_OUTPUTS, set_weights=True)\n",
    "test_backward(Linear(OUTPUT_DIM, INPUT_DIM), LINEAR_D_INPUTS, LINEAR_D_OUTPUTS, LINEAR_MEAD_D_WEIGHTS)\n",
    "\n",
    "test_forward(ReLU(INPUT_DIM), RELU_OUTPUTS, set_weights=True)\n",
    "test_backward(ReLU(INPUT_DIM), RELU_D_INPUTS, RELU_D_OUTPUTS)\n",
    "\n",
    "test_forward(Softmax(INPUT_DIM), SOFTMAX_OUTPUTS, set_weights=True)\n",
    "test_backward(Softmax(INPUT_DIM), SOFTMAX_D_INPUTS, SOFTMAX_D_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOWDG_qZER__"
   },
   "source": [
    "## Часть 2. Обучение нейросети стохастическим градиентным спуском"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_JP-d1QESAA"
   },
   "source": [
    "### Теоретическая часть \n",
    "#### Градиентный спуск\n",
    "Настройка весов нейросети может быть сведена к поиску вектора $w$, доставляющего минимум функционалу ошибки:\n",
    "\n",
    "$$E(w) = E( \\boldsymbol{y},  \\boldsymbol{\\widehat{y}}) \\to min $$\n",
    "\n",
    "Минимизировать E(w) будем с помощью градиентного спуска. Правило изменения вектора весов на каждой итерации: \n",
    "\n",
    "$$\n",
    "w^{(k)} = w^{(k - 1)} - \\beta \\nabla_w E(w^{(k - 1)})\n",
    "$$\n",
    "Длину шага $\\beta > 0$ в рамках данного задания предлагается брать равной некоторой малой константе.\n",
    "\n",
    "В случае полного градиентного спуска $\\nabla_w E(w)$ считается все объекты выборки). В случае градиентного спуска  с минибатчем \n",
    "$$\\nabla_w E(w) \\approx \\frac{1}{n}\\sum_{j=1}^{n}{\\nabla_w q_{i_{k_j}} (w)}$$\n",
    "где $q_{i_{k_j}}$ — случайно выбранные номера слагаемых, а n меньше общего колличества примеров для обучения.\n",
    "\n",
    "#### Момент импульса(momentum)\n",
    "Может оказаться, что направление антиградиента сильно меняется от шага к шагу. Чтобы добиться болле эффективной сходимости, можно усреднять векторы антиградиента с нескольких предыдущих шагов — в этом случае шум уменьшится, и такой средний вектор будет указывать в сторону общего направления движения. Введем вектор инерции:\n",
    "    $$h_0 = 0;$$\n",
    "    $$h_k = \\alpha h_{k - 1} + \\beta \\nabla_w E(w^{(k - 1)}) $$\n",
    "Тогда шаг градиентного спуска будет:\n",
    "    $$w^{(k)} = w^{(k - 1)}  - h_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ugrt8NpIESAC"
   },
   "source": [
    "### Практическая часть\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9RSv44XESAD"
   },
   "source": [
    "**Задание 3.** Реализация стохастического градиентного спуска с моментом\n",
    "\n",
    "\n",
    "Вам необходимо реализовать алгоритм обновления весов с моментом. Для этого необходимо реализовать метод `update_weights` класса `SGD`. Также вам необходимо реализовать метод `update_weights` во всех слоях нейросети(у которых есть веса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMNo7YahESAE"
   },
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, lr, momentum=0):\n",
    "        self._lr = lr\n",
    "        self._momentum = 0\n",
    "        self._alfa = momentum\n",
    "\n",
    "    def update_weights(self, weights, gradient):\n",
    "        \"\"\"\n",
    "        weights: np.array((n, m)), current weigths of algorithm\n",
    "        gradient: np.array((n, m)), average gradient by weights\n",
    "        ---\n",
    "        output: np.array((n, m)), new weights values\n",
    "        \"\"\"\n",
    "        self._momentum = self._alfa*self._momentum + self._lr*gradient \n",
    "        return weights - self._momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wa-IHYkNESAI"
   },
   "source": [
    "**Задание 4.** Обучение нейросети для задачи классификации цифр mnist\n",
    "\n",
    "В этой части вам необходимо обучить вашу нейросеть для задачи классификации рукописных цифр mnist. Вам потребуются методы `fit` и `evaluate` класса `Model`. Можете использовать предложенную архитектуру или выбрать любую другую. Шаг обучения, количество эпох и размер батча выбирите на ваше усмотрение. Посчитайте ошибку и точность(accuracy) предсказания на тестовом датасете.\n",
    "\n",
    "Обучите нейросеть без момента и c моментом(выбранным на ваше усмотрение) и постройте графики ошибки во время обучения в зависимости от числа итераций. Постройте те же графики для валидационной части. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "k4cCZ2_qESAK",
    "outputId": "c35d30fb-004f-4ad5-9bcb-b3c15edf413c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.14.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install mnist\n",
    "import mnist\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk79FGI2ESAO"
   },
   "outputs": [],
   "source": [
    "def prepare_data(images, labels):\n",
    "    binarizer = LabelBinarizer()\n",
    "    X = images.reshape((images.shape[0], 28 * 28))\n",
    "    y = binarizer.fit_transform(labels)\n",
    "    return X / 255.0, y \n",
    "\n",
    "def create_mnist_model(loss, optimizer):\n",
    "    model = Model(loss=loss, optimizer=optimizer)\n",
    "    model.add(Linear(10, input_dim=28*28))\n",
    "    model.add(ReLU())\n",
    "    model.add(Linear(10))\n",
    "    model.add(Softmax())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ey_gvBy5ESAS"
   },
   "outputs": [],
   "source": [
    "X, y = prepare_data(mnist.train_images(), mnist.train_labels())\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)\n",
    "X_test, y_test = prepare_data(mnist.test_images(), mnist.test_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1309
    },
    "colab_type": "code",
    "id": "N715WVGtESAY",
    "outputId": "b5d6bd3e-9a28-40fa-8cd0-d6a7d167f928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model without moments, lr = 0.2,batch_size = 8,epochs = 15:\n",
      "Epoch: 1 train loss: [0.1293354] val loss: [0.12941014]\n",
      "Epoch: 2 train loss: [0.08183887] val loss: [0.08116195]\n",
      "Epoch: 3 train loss: [0.06209409] val loss: [0.06129698]\n",
      "Epoch: 4 train loss: [0.0534416] val loss: [0.05271894]\n",
      "Epoch: 5 train loss: [0.04837014] val loss: [0.04760922]\n",
      "Epoch: 6 train loss: [0.04492472] val loss: [0.04437247]\n",
      "Epoch: 7 train loss: [0.04231874] val loss: [0.04183849]\n",
      "Epoch: 8 train loss: [0.04034328] val loss: [0.03989769]\n",
      "Epoch: 9 train loss: [0.03886806] val loss: [0.0385962]\n",
      "Epoch: 10 train loss: [0.0374628] val loss: [0.03715691]\n",
      "Epoch: 11 train loss: [0.03644488] val loss: [0.03618626]\n",
      "Epoch: 12 train loss: [0.03547033] val loss: [0.03533036]\n",
      "Epoch: 13 train loss: [0.03467953] val loss: [0.03464513]\n",
      "Epoch: 14 train loss: [0.03408866] val loss: [0.03416184]\n",
      "Epoch: 15 train loss: [0.03349783] val loss: [0.03346745]\n",
      "Loss:  [0.03240001] | accuracy :  0.9075\n",
      "Training model with moment = 0.9, lr = 0.2,batch_size = 8,epochs = 15:\n",
      "Epoch: 1 train loss: [0.13420455] val loss: [0.13280637]\n",
      "Epoch: 2 train loss: [0.0818692] val loss: [0.08111295]\n",
      "Epoch: 3 train loss: [0.06143168] val loss: [0.06087201]\n",
      "Epoch: 4 train loss: [0.05243722] val loss: [0.05197406]\n",
      "Epoch: 5 train loss: [0.04726361] val loss: [0.04700903]\n",
      "Epoch: 6 train loss: [0.04378615] val loss: [0.04352827]\n",
      "Epoch: 7 train loss: [0.0412264] val loss: [0.04108206]\n",
      "Epoch: 8 train loss: [0.03923067] val loss: [0.03907398]\n",
      "Epoch: 9 train loss: [0.03756119] val loss: [0.03753909]\n",
      "Epoch: 10 train loss: [0.0362186] val loss: [0.03620752]\n",
      "Epoch: 11 train loss: [0.03510732] val loss: [0.03516035]\n",
      "Epoch: 12 train loss: [0.03414695] val loss: [0.03419352]\n",
      "Epoch: 13 train loss: [0.03337017] val loss: [0.03341917]\n",
      "Epoch: 14 train loss: [0.03260384] val loss: [0.03274631]\n",
      "Epoch: 15 train loss: [0.03193751] val loss: [0.03218978]\n",
      "Loss:  [0.03107191] | accuracy :  0.9126\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAALKCAYAAAAWBBYmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc72fc9a0f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_without_moment = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.2))\n",
    "model_with_moment = create_mnist_model(loss=CategoricalCrossentropy(), optimizer=SGD(lr=0.2,momentum = 0.9))\n",
    "\n",
    "print(\"Training model without moments, lr = 0.2,batch_size = 8,epochs = 15:\")\n",
    "model_without_moment.fit( X_train, y_train, batch_size = 10, epochs = 15,X_val = X_val,Y_val = y_val)\n",
    "loss_without_moment = model_without_moment.evaluate(X_test,y_test,batch_size = 32)\n",
    "print(\"Loss: \",loss_without_moment,\"| accuracy : \",model_without_moment._accuracy)\n",
    "\n",
    "print(\"Training model with moment = 0.9, lr = 0.2,batch_size = 8,epochs = 15:\")\n",
    "model_with_moment.fit( X_train, y_train, batch_size = 10, epochs = 15,X_val = X_val,Y_val = y_val)\n",
    "loss_with_moment = model_with_moment.evaluate(X_test,y_test,batch_size = 32)\n",
    "print(\"Loss: \",loss_with_moment,\"| accuracy : \", model_with_moment._accuracy)\n",
    "\n",
    "gridsize = (2, 2)\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax1 = plt.subplot2grid(gridsize, (0, 0))\n",
    "ax2 = plt.subplot2grid(gridsize, (1, 0))\n",
    "ax3 = plt.subplot2grid(gridsize, (0, 1))\n",
    "ax4 = plt.subplot2grid(gridsize, (1, 1))\n",
    "\n",
    "arr = range(len(model_without_moment.loss_val_history))\n",
    "ax1.scatter(x=arr, y=model_without_moment.loss_train_history)\n",
    "ax1.set_title('Scatter: model without moment, loss of iteration')\n",
    "ax1.set_xlabel('$iter$')\n",
    "ax1.set_ylabel('$val$')\n",
    "ax2.scatter(x=arr, y=model_without_moment.loss_val_history)\n",
    "ax2.set_title('Scatter: model without moment, loss val of iteration')\n",
    "ax2.set_xlabel('$iter$')\n",
    "ax2.set_ylabel('$val$')\n",
    "\n",
    "arr = range(len(model_with_moment.loss_val_history))\n",
    "ax3.scatter(x=arr, y=model_with_moment.loss_train_history)\n",
    "ax3.set_title('Scatter: model with moment, loss of iteration')\n",
    "ax3.set_xlabel('$iter$')\n",
    "ax3.set_ylabel('$val$')\n",
    "ax4.scatter(x=arr, y=model_with_moment.loss_val_history)\n",
    "ax4.set_title('Scatter: model with moment, loss val of iteration')\n",
    "ax4.set_xlabel('$iter$')\n",
    "ax4.set_ylabel('$val$')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUyOk_0OESAl"
   },
   "outputs": [],
   "source": [
    "Выводы: Из вышеизложенных графиков мы видим, что модель, обученная SGD с моментами, теряет loss быстрее, чем SGD без моментов.\n",
    "Связано это с тем, что когда функция попадает в “овраг”, т.е. по одному из направлений имеем быстрый спуск, а по другому медленный,\n",
    "то SGD без моментов приводит к крайне медленной сходимости к минимуму.\n",
    "При этом конечные точности SGD с моментами и без уравниваются.Поэтому для достижения нужной точности в SGD с моментами требуется меньшее кол-во эпох."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "backprop.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
